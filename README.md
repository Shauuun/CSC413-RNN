# CSC413-RNN

Introduction: 
The deep learning model we built is an RNN with self-attention mechanism that is used for text classification. The task being solved is to classify the reviews which are inputs, into a set of categories. The input is a sequence of words represented as word embeddings, and the output is a probability distribution over the categories. In the model, we used an LSTM to learn sequential dependencies in the input text with a self-attention mechanism to weigh the importance of each word in the input sequence. The output is generated by passing the final hidden state of the LSTM through two fully connected layers with ReLU activation. Then, a final softmax layer is used to obtain the category probabilities.

Model Figure:

Model Parameters:

Model Examples:

Data Source:
The souce of our data set comes from a Yelp dataset(https://www.yelp.com/dataset). The dataset contains over 6 million reviews, along with business and user information.

Data Summary:

Data Transformation:

Data Split:
We used a 60/20/20 training, validation, testing split. We used this split because we wanted a large dataset for training to identify details and patterns. The 20% validation is sufficient to fine tune and the 20% testing is sufficient to evaluate performance.

Training Curve:

Hyperparameter Tuning:

Quantitative Measures:

Quantitative and Qualitative Results:

Justification of Results:

Ethical Consideration:
Some ethical issues could be the distribution association. For example if the training set has many negative reviews for in a certain location, it could associate that neighborhood with negative reviews. Also, since the model can only classify reviews as positive or negative, it could potentially not encompass the full idea of the review. 

Authors: